{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neuralist/DS&ML/Data_Science/MIPT_Data_Scientist/DA & ML/UFC-predictor/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, log_loss\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('ufc-master-for-binary-classification_featured.csv', index_col='Unnamed: 0') #drop(columns=['RedFighter', 'BlueFighter', 'Result'], axis=1)\n",
    "y = pd.read_csv('ufc-master-for-binary-classification.csv', index_col='Unnamed: 0')['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "1995    0\n",
       "1996    1\n",
       "1997    1\n",
       "1998    0\n",
       "1999    1\n",
       "Name: Result, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_2000_featured.csv', index_col='Unnamed: 0') #.loc[:1700, :]\n",
    "y = pd.read_csv('y_2000.csv', index_col='Unnamed: 0') #.loc[:1700, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]], shape=(2630, 1))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.iloc[:1900, :]\n",
    "# y = y.iloc[:1900]\n",
    "\n",
    "\n",
    "test_size = int(len(X)*0.1)\n",
    "\n",
    "X_train = X.iloc[test_size:, :]\n",
    "y_train = y.iloc[test_size:]\n",
    "X_test = X.iloc[:test_size, :]\n",
    "y_test = y.iloc[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =np.array(y_train)\n",
    "y_test =np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабируйте данные\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель случайного леса\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=folds, scoring='f1', verbose=True, n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Лучшие гиперпараметры для случайного леса: {grid_search_rf.best_params_}\")\n",
    "print(f\"Лучший результат для случайного леса: {grid_search_rf.best_score_}\")\n",
    "\n",
    "# Используйте лучшую модель для предсказания\n",
    "y_pred_rf_best = grid_search_rf.best_estimator_.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим метрики качества модели\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf_best))\n",
    "\n",
    "# Матрица ошибок\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf_best),  annot=True, fmt=\"d\", cmap='Greens')\n",
    "plt.ylabel('Real value')\n",
    "plt.xlabel('predicted value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Определение гиперпараметров для оптимизации\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    # Создание модели\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Обучение модели\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Оценка модели\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Создание Bayesian Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Запуск Bayesian Optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Вывод лучших гиперпараметров и результата\n",
    "print(\"Лучшие гиперпараметры:\", study.best_params)\n",
    "print(\"Результат:\", study.best_value)\n",
    "\n",
    "# Создание модели с лучшими гиперпараметрами\n",
    "best_rf = RandomForestClassifier(**study.best_params)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Оценка модели с лучшими гиперпараметрами\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "print(\"F1-мера модели с лучшими гиперпараметрами:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим метрики качества модели\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred),  annot=True, fmt=\"d\", cmap='Greens')\n",
    "plt.ylabel('Real value')\n",
    "plt.xlabel('predicted value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Определение гиперпараметров для оптимизации\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 16)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 2, 100)\n",
    "    gamma = trial.suggest_loguniform('gamma', 0.01, 10.0)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.2, 1.0)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 0.01, 10.0)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 0.01, 10.0)\n",
    "\n",
    "    # Создание модели\n",
    "    model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate,\n",
    "                              n_estimators=n_estimators, gamma=gamma,\n",
    "                              subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                              reg_alpha=reg_alpha, reg_lambda=reg_lambda)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Оценка модели\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Создание Bayesian Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Запуск Bayesian Optimization\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Вывод лучших гиперпараметров и результата\n",
    "# print(\"Лучшие гиперпараметры:\", study.best_params)\n",
    "# print(\"Результат:\", study.best_value)\n",
    "\n",
    "# Создание модели с лучшими гиперпараметрами\n",
    "best_model = xgb.XGBClassifier(**study.best_params)\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Оценка модели с лучшими гиперпараметрами\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(\"F1-мера модели с лучшими гиперпараметрами:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "# Вывод параметров лучшей модели\n",
    "print(\"Параметры лучшей модели:\", best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим метрики качества модели\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred),  annot=True, fmt=\"d\", cmap='Greens')\n",
    "plt.ylabel('Real value')\n",
    "plt.xlabel('predicted value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Определение нейросети\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Параметры модели\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Обратный проход и оптимизация\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/DS&ML/Data_Science/MIPT_Data_Scientist/DA & ML/UFC-predictor/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS&ML/Data_Science/MIPT_Data_Scientist/DA & ML/UFC-predictor/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DS&ML/Data_Science/MIPT_Data_Scientist/DA & ML/UFC-predictor/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS&ML/Data_Science/MIPT_Data_Scientist/DA & ML/UFC-predictor/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Прямой проход\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратный проход и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = y_test_tensor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Вычисление F1-меры\n",
    "    f1 = f1_score(y_test_tensor, predicted, average='weighted')\n",
    "    print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для аугментации данных\n",
    "def augment_data(X, y, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, X.shape)\n",
    "    X_augmented = X + noise\n",
    "    y_augmented = y.copy()\n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "# Аугментация тренировочных данных\n",
    "X_train_augmented, y_train_augmented = augment_data(X_train, y_train)\n",
    "\n",
    "# Преобразование аугментированных данных в тензоры\n",
    "X_train_augmented_tensor = torch.tensor(X_train_augmented, dtype=torch.float32)\n",
    "# y_train_augmented_tensor = torch.tensor(y_train_augmented.values, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader для аугментированных данных\n",
    "train_dataset_augmented = TensorDataset(X_train_augmented_tensor, y_train_tensor)\n",
    "train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Обучение модели на аугментированных данных\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader_augmented):\n",
    "        # Прямой проход\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратный проход и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели после обучения на аугментированных данных\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = y_test_tensor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy after augmentation: {accuracy:.4f}')\n",
    "\n",
    "    # Вычисление F1-меры\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_test_tensor, predicted, average='weighted')\n",
    "    print(f'F1 Score after augmentation: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет + аугментированный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Стандартизация данных\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Определение нейросети\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Параметры модели\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader для базовых данных\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Функция для аугментации данных\n",
    "def augment_data(X,noise_level=0.03):\n",
    "    noise = np.random.normal(0, noise_level, X.shape)\n",
    "    X_augmented = X + noise  \n",
    "    return X_augmented\n",
    "\n",
    "# Аугментация тренировочных данных\n",
    "X_train_augmented = augment_data(X_train)\n",
    "\n",
    "# Преобразование аугментированных данных в тензоры\n",
    "X_train_augmented_tensor = torch.tensor(X_train_augmented, dtype=torch.float32)\n",
    "\n",
    "# Создание DataLoader для аугментированных данных\n",
    "train_dataset_augmented = TensorDataset(X_train_augmented_tensor, y_train_tensor)\n",
    "train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение модели на базовых данных\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Прямой проход\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратный проход и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Оценка модели после обучения на базовых данных\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = y_test_tensor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy after training on base data: {accuracy:.4f}')\n",
    "\n",
    "    # Вычисление F1-меры\n",
    "    f1 = f1_score(y_test_tensor, predicted, average='weighted')\n",
    "    print(f'F1 Score after training on base data: {f1:.4f}')\n",
    "\n",
    "# Обучение модели на аугментированных данных\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader_augmented):\n",
    "        # Прямой проход\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратный проход и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Оценка модели после обучения на аугментированных данных\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = y_test_tensor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy after training on augmented data: {accuracy:.4f}')\n",
    "\n",
    "    # Вычисление F1-меры\n",
    "    f1 = f1_score(y_test_tensor, predicted, average='weighted')\n",
    "    print(f'F1 Score after training on augmented data: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
